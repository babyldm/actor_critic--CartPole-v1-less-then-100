import random
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import Adam
import gym
def all_nn(
    yinput=[[4,1],[4,1]],
    layer_out=[[6,2],[6,1]],
    layers=[[Dense,Dense],[Dense,Dense]],
    l_init=[['lecun_uniform']*2,['lecun_uniform']*2],
    l_activa=[["relu","softmax"],["relu","softmax"]],
    l_loss=['mse','mse'],
    l_optimizer=[Adam(),Adam()],
    name1=['actor','critic']):
    names=locals()
    for i,c in enumerate(name1):
        names[c]=Sequential()
        names[c].add(layers[i][0](layer_out[i][0], kernel_initializer=l_init[i][0], input_shape=(yinput[i][0],)))
        names[c].add(Activation(l_activa[i][0]))
        for layer, init, node, activation in list(zip(layers[i],l_init[i], layer_out[i], l_activa[i]))[1:]:
            names[c].add(layer(node, kernel_initializer=init, input_shape=(node,)))
            names[c].add(Activation(activation))
        names[c].compile(loss=l_loss[i], optimizer=l_optimizer[i], metrics=['accuracy'])#'mse'
    return names[name1[0]],names[name1[1]]
class Agent:
    def __init__(self,state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.actor,self.critic  = all_nn(yinput=[[state_size,1],[state_size,1]])
        self.epsilon = 1
        self.gamma = 0.96
    def select_move(self, state):
        if 1-self.epsilon > random.random():
            return np.argmax(self.actor.predict(state, batch_size=32, verbose=0))
        return random.randrange(self.action_size)
    def eval(self, state):
        return self.critic.predict(state, verbose=0)
    def update(self, state, action, true_reward):
        actor_reward = self.actor.predict(state)
        actor_reward[0][action] = true_reward
        self.critic.fit(state, true_reward, verbose=0)
        self.actor.fit(state, actor_reward, verbose=0)
        self.epsilon*=0.999
def main999():
    EPISODES = 5000#5000
    env = gym.make('CartPole-v1')
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    agent = Agent(state_size,action_size)
    done = False
    times=1
    for e in range(EPISODES):
        for time in range(500):
            state = env.reset()
            env.render()
            action = agent.select_move(np.matrix(state))
            state, reward, done, _ = env.step(action)
            while not done:
                action = agent.select_move(np.matrix(state))
                next_state, reward, done , _ = env.step(action)
                agent.update(np.matrix(state), action, reward + agent.gamma * agent.eval(np.matrix(next_state)))
                state = next_state
               [ print(times) & times =1 if done]
main999()
